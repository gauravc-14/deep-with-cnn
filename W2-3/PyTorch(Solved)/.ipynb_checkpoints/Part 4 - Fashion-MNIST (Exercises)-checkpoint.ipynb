{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Fashion-MNIST\n",
    "\n",
    "Now it's your turn to build and train a neural network. You'll be using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), a drop-in replacement for the MNIST dataset. MNIST is actually quite trivial with neural networks where you can easily achieve better than 97% accuracy. Fashion-MNIST is a set of 28x28 greyscale images of clothes. It's more complex than MNIST, so it's a better representation of the actual performance of your network, and a better representation of datasets you'll use in the real world.\n",
    "\n",
    "<img src='assets/fashion-mnist-sprite.png' width=500px>\n",
    "\n",
    "In this notebook, you'll build your own neural network. For the most part, you could just copy and paste the code from Part 3, but you wouldn't be learning. It's important for you to write the code yourself and get it to work. Feel free to consult the previous notebooks though as you work through this.\n",
    "\n",
    "First off, let's load the dataset through torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchvision import datasets, transforms\n",
    "# import helper\n",
    "\n",
    "# # Define a transform to normalize the data\n",
    "# transform = transforms.Compose([transforms.ToTensor(),\n",
    "#                                 transforms.Normalize((0.5,), (0.5,))])\n",
    "# # Download and load the training data\n",
    "# trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# # Download and load the test data\n",
    "# testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0        30        43         0   \n",
       "3       0  ...         3         0         0         0         0         1   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    6000\n",
       "8    6000\n",
       "7    6000\n",
       "6    6000\n",
       "5    6000\n",
       "4    6000\n",
       "3    6000\n",
       "2    6000\n",
       "1    6000\n",
       "0    6000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train['label']\n",
    "x_train = train.drop(['label'], axis = 1)\n",
    "y_test = test['label']\n",
    "x_test = test.drop(['label'], axis = 1)\n",
    "\n",
    "y_train.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   4.,   0.,   0.,   0.,   0.,   0.,  62.,  61.,\n",
       "          21.,  29.,  23.,  51., 136.,  61.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  88.,\n",
       "         201., 228., 225., 255., 115.,  62., 137., 255., 235., 222., 255., 135.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,  47., 252., 234., 238., 224., 215., 215., 229., 108., 180., 207.,\n",
       "         214., 224., 231., 249., 254.,  45.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   1.,   0.,   0., 214., 222., 210., 213., 224., 225., 217.,\n",
       "         220., 254., 233., 219., 221., 217., 223., 221., 240., 254.,   0.,   0.,\n",
       "           1.,   0.,   0.,   0.,   1.,   0.,   0.,   0., 128., 237., 207., 224.,\n",
       "         224., 207., 216., 214., 210., 208., 211., 221., 208., 219., 213., 226.,\n",
       "         211., 237., 150.,   0.,   0.,   0.,   0.,   0.,   0.,   2.,   0.,   0.,\n",
       "         237., 222., 215., 207., 210., 212., 213., 206., 214., 213., 214., 213.,\n",
       "         210., 215., 214., 206., 199., 218., 255.,  13.,   0.,   2.,   0.,   0.,\n",
       "           0.,   4.,   0.,  85., 228., 210., 218., 200., 211., 208., 203., 215.,\n",
       "         210., 209., 209., 210., 213., 211., 210., 217., 206., 213., 231., 175.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0., 217., 224., 215., 206., 205.,\n",
       "         204., 217., 230., 222., 215., 224., 233., 228., 232., 228., 224., 207.,\n",
       "         212., 215., 213., 229.,  31.,   0.,   4.,   0.,   1.,   0.,  21., 225.,\n",
       "         212., 212., 203., 211., 225., 193., 139., 136., 195., 147., 156., 139.,\n",
       "         128., 162., 197., 223., 207., 220., 213., 232., 177.,   0.,   0.,   0.,\n",
       "           0.,   0., 123., 226., 207., 211., 209., 205., 228., 158.,  90., 103.,\n",
       "         186., 138., 100., 121., 147., 158., 183., 226., 208., 214., 209., 216.,\n",
       "         255.,  13.,   0.,   1.,   0.,   0., 226., 219., 202., 208., 206., 205.,\n",
       "         216., 184., 156., 150., 193., 170., 164., 168., 188., 186., 200., 219.,\n",
       "         216., 213., 213., 211., 233., 148.,   0.,   0.,   0.,  45., 227., 204.,\n",
       "         214., 211., 218., 222., 221., 230., 229., 221., 213., 224., 233., 226.,\n",
       "         220., 219., 221., 224., 223., 217., 210., 218., 213., 254.,   0.,   0.,\n",
       "           0., 157., 226., 203., 207., 211., 209., 215., 205., 198., 207., 208.,\n",
       "         201., 201., 197., 203., 205., 210., 207., 213., 214., 214., 214., 213.,\n",
       "         208., 234., 107.,   0.,   0., 235., 213., 204., 211., 210., 209., 213.,\n",
       "         202., 197., 204., 215., 217., 213., 212., 210., 206., 212., 203., 211.,\n",
       "         218., 215., 214., 208., 209., 222., 230.,   0.,  52., 255., 207., 200.,\n",
       "         208., 213., 210., 210., 208., 207., 202., 201., 209., 216., 216., 216.,\n",
       "         216., 214., 212., 205., 215., 201., 228., 208., 214., 212., 218.,  25.,\n",
       "         118., 217., 201., 206., 208., 213., 208., 205., 206., 210., 211., 202.,\n",
       "         199., 207., 208., 209., 210., 207., 210., 210., 245., 139., 119., 255.,\n",
       "         202., 203., 236., 114., 171., 238., 212., 203., 220., 216., 217., 209.,\n",
       "         207., 205., 210., 211., 206., 204., 206., 209., 211., 215., 210., 206.,\n",
       "         221., 242.,   0., 224., 234., 230., 181.,  26.,  39., 145., 201., 255.,\n",
       "         157., 115., 250., 200., 207., 206., 207., 213., 216., 206., 205., 206.,\n",
       "         207., 206., 215., 207., 221., 238.,   0.,   0., 188.,  85.,   0.,   0.,\n",
       "           0.,   0.,   0.,  31.,   0., 129., 253., 190., 207., 208., 208., 208.,\n",
       "         209., 211., 211., 209., 209., 209., 212., 201., 226., 165.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   2.,   0.,   0.,   0.,   0.,  89., 254., 199.,\n",
       "         199., 192., 196., 198., 199., 201., 202., 203., 204., 203., 203., 200.,\n",
       "         222., 155.,   0.,   3.,   3.,   3.,   2.,   0.,   0.,   0.,   1.,   5.,\n",
       "           0.,   0., 255., 218., 226., 232., 228., 224., 222., 220., 219., 219.,\n",
       "         217., 221., 220., 212., 236.,  95.,   0.,   2.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0., 155., 194., 168., 170., 171., 173.,\n",
       "         173., 179., 177., 175., 172., 171., 167., 161., 180.,   0.,   0.,   1.,\n",
       "           0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = torch.tensor(x_train.values.astype(np.float32)) \n",
    "train_y = torch.tensor(y_train.values.astype(np.int64))\n",
    "train_x[0], train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.tensor(x_test.values.astype(np.float32))\n",
    "test_y = torch.tensor(y_test.values.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self,i):\n",
    "        item = self.data[i]\n",
    "        label = self.labels[i]\n",
    "        return (item,label)\n",
    "    \n",
    "training_data = mydataset(train_x,train_y)\n",
    "testing_data = mydataset(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size = 100, shuffle = True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits or log-softmax from the forward pass. It's up to you how many layers you add and the size of those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your network architecture here\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sequence = nn.Sequential(\n",
    "            nn.Linear(784,100),\n",
    "            nn.BatchNorm1d(num_features = 100), # normalize inputs across all the batches\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4), #DropOut minimise the overfitting on training set\n",
    "            nn.Linear(100,64),\n",
    "            nn.BatchNorm1d(num_features = 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,28),\n",
    "            nn.BatchNorm1d(num_features = 28),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(28,10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        output = self.sequence(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "\n",
    "Now you should create your network and train it. First you'll want to define [the criterion](http://pytorch.org/docs/master/nn.html#loss-functions) ( something like `nn.CrossEntropyLoss`) and [the optimizer](http://pytorch.org/docs/master/optim.html) (typically `optim.SGD` or `optim.Adam`).\n",
    "\n",
    "Then write the training code. Remember the training pass is a fairly straightforward process:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "By adjusting the hyperparameters (hidden units, learning rate, etc), you should be able to get the training loss below 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the network, define the criterion and optimizer\n",
    "model = NN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 100\n",
    "lr_rate = .005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = lr_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the network here\n",
    "def train_loop(dataloader, model, loss_fn, optimiser):\n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss, correct = 0.0, 0\n",
    "    for X, y in dataloader:\n",
    "        pred = model(X)\n",
    "        loss = criterion(pred, y)\n",
    "        train_loss += loss.item() # Adding loss across all batches\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        #Adding Accuracy across all batches\n",
    "        \n",
    "    train_loss /= size #mean of loss\n",
    "    correct /= size  #mean of accuracy     \n",
    "    print(f\"Train Loss: {train_loss:.3f}\")\n",
    "    print(f\"Train Accuracy: {correct:.3f}\")\n",
    "    return train_loss\n",
    "    \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += criterion(pred, y).item()\n",
    "            _, predicted = torch.max(pred, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            \n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Loss: {test_loss:.3f}\")\n",
    "    print(f\"Test Accuracy: {correct:.3f}\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================== Epoch: 0 ==================]\n",
      "Train Loss: 0.007\n",
      "Train Accuracy: 0.753\n",
      "\n",
      "Test Loss: 0.006\n",
      "Test Accuracy: 0.810\n",
      "[=================== Epoch: 1 ==================]\n",
      "Train Loss: 0.005\n",
      "Train Accuracy: 0.815\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.829\n",
      "[=================== Epoch: 2 ==================]\n",
      "Train Loss: 0.005\n",
      "Train Accuracy: 0.830\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.835\n",
      "[=================== Epoch: 3 ==================]\n",
      "Train Loss: 0.005\n",
      "Train Accuracy: 0.838\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.842\n",
      "[=================== Epoch: 4 ==================]\n",
      "Train Loss: 0.005\n",
      "Train Accuracy: 0.842\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.841\n",
      "[=================== Epoch: 5 ==================]\n",
      "Train Loss: 0.005\n",
      "Train Accuracy: 0.846\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.842\n",
      "[=================== Epoch: 6 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.850\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.844\n",
      "[=================== Epoch: 7 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.851\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.847\n",
      "[=================== Epoch: 8 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.853\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.847\n",
      "[=================== Epoch: 9 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.856\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.845\n",
      "[=================== Epoch: 10 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.858\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.849\n",
      "[=================== Epoch: 11 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.860\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.849\n",
      "[=================== Epoch: 12 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.863\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.848\n",
      "[=================== Epoch: 13 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.862\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.852\n",
      "[=================== Epoch: 14 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.865\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.855\n",
      "[=================== Epoch: 15 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.867\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.857\n",
      "[=================== Epoch: 16 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.869\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.852\n",
      "[=================== Epoch: 17 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.869\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.854\n",
      "[=================== Epoch: 18 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.870\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.855\n",
      "[=================== Epoch: 19 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.871\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.856\n",
      "[=================== Epoch: 20 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.873\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.856\n",
      "[=================== Epoch: 21 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.872\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.856\n",
      "[=================== Epoch: 22 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.874\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.859\n",
      "[=================== Epoch: 23 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.875\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.860\n",
      "[=================== Epoch: 24 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.872\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.858\n",
      "[=================== Epoch: 25 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.876\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.858\n",
      "[=================== Epoch: 26 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.876\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.857\n",
      "[=================== Epoch: 27 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.878\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.858\n",
      "[=================== Epoch: 28 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.876\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.860\n",
      "[=================== Epoch: 29 ==================]\n",
      "Train Loss: 0.004\n",
      "Train Accuracy: 0.878\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.864\n",
      "[=================== Epoch: 30 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.879\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.860\n",
      "[=================== Epoch: 31 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.880\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.858\n",
      "[=================== Epoch: 32 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.881\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.860\n",
      "[=================== Epoch: 33 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.880\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.864\n",
      "[=================== Epoch: 34 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.881\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.860\n",
      "[=================== Epoch: 35 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.882\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.858\n",
      "[=================== Epoch: 36 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.880\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.861\n",
      "[=================== Epoch: 37 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.884\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.859\n",
      "[=================== Epoch: 38 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.882\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.857\n",
      "[=================== Epoch: 39 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.883\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.863\n",
      "[=================== Epoch: 40 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.884\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.860\n",
      "[=================== Epoch: 41 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.884\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.862\n",
      "[=================== Epoch: 42 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.886\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.863\n",
      "[=================== Epoch: 43 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.884\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.868\n",
      "[=================== Epoch: 44 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.888\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.863\n",
      "[=================== Epoch: 45 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.887\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.859\n",
      "[=================== Epoch: 46 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.887\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.863\n",
      "[=================== Epoch: 47 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.889\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.862\n",
      "[=================== Epoch: 48 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.886\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.866\n",
      "[=================== Epoch: 49 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.888\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.861\n",
      "[=================== Epoch: 50 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.887\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.859\n",
      "[=================== Epoch: 51 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.889\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.865\n",
      "[=================== Epoch: 52 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.890\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.859\n",
      "[=================== Epoch: 53 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.891\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.861\n",
      "[=================== Epoch: 54 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.890\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.860\n",
      "[=================== Epoch: 55 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.891\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.862\n",
      "[=================== Epoch: 56 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.891\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.861\n",
      "[=================== Epoch: 57 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.892\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.868\n",
      "[=================== Epoch: 58 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.891\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.866\n",
      "[=================== Epoch: 59 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.891\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.865\n",
      "[=================== Epoch: 60 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.893\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.863\n",
      "[=================== Epoch: 61 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.894\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.864\n",
      "[=================== Epoch: 62 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.892\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.863\n",
      "[=================== Epoch: 63 ==================]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.892\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.861\n",
      "[=================== Epoch: 64 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.893\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.859\n",
      "[=================== Epoch: 65 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.895\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.867\n",
      "[=================== Epoch: 66 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.893\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.868\n",
      "[=================== Epoch: 67 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.893\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.863\n",
      "[=================== Epoch: 68 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.895\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.862\n",
      "[=================== Epoch: 69 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.895\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.861\n",
      "[=================== Epoch: 70 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.893\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.863\n",
      "[=================== Epoch: 71 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.895\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.868\n",
      "[=================== Epoch: 72 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.895\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.869\n",
      "[=================== Epoch: 73 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.896\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.863\n",
      "[=================== Epoch: 74 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.896\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.865\n",
      "[=================== Epoch: 75 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.897\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.862\n",
      "[=================== Epoch: 76 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.895\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.868\n",
      "[=================== Epoch: 77 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.898\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.865\n",
      "[=================== Epoch: 78 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.897\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.868\n",
      "[=================== Epoch: 79 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.896\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.867\n",
      "[=================== Epoch: 80 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.896\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.867\n",
      "[=================== Epoch: 81 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.899\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.863\n",
      "[=================== Epoch: 82 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.897\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.865\n",
      "[=================== Epoch: 83 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.897\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.864\n",
      "[=================== Epoch: 84 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.898\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.863\n",
      "[=================== Epoch: 85 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.898\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.865\n",
      "[=================== Epoch: 86 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.898\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.864\n",
      "[=================== Epoch: 87 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.901\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.868\n",
      "[=================== Epoch: 88 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.900\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.862\n",
      "[=================== Epoch: 89 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.899\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.864\n",
      "[=================== Epoch: 90 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.901\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.864\n",
      "[=================== Epoch: 91 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.902\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.865\n",
      "[=================== Epoch: 92 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.901\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.864\n",
      "[=================== Epoch: 93 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.900\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.866\n",
      "[=================== Epoch: 94 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.899\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.860\n",
      "[=================== Epoch: 95 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.900\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.868\n",
      "[=================== Epoch: 96 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.901\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.866\n",
      "[=================== Epoch: 97 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.901\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.862\n",
      "[=================== Epoch: 98 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.901\n",
      "\n",
      "Test Loss: 0.004\n",
      "Test Accuracy: 0.864\n",
      "[=================== Epoch: 99 ==================]\n",
      "Train Loss: 0.003\n",
      "Train Accuracy: 0.902\n",
      "\n",
      "Test Loss: 0.005\n",
      "Test Accuracy: 0.866\n"
     ]
    }
   ],
   "source": [
    "total_train_loss = []\n",
    "total_test_loss = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"[=================== Epoch: {t} ==================]\")\n",
    "    train_los = train_loop(train_dataloader, model, criterion, optimizer)\n",
    "    total_train_loss.append(train_los)\n",
    "    print('')\n",
    "    test_los = test_loop(test_dataloader, model, criterion)\n",
    "    total_test_loss.append(test_los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(total_train_loss)\n",
    "plt.plot(total_test_loss)\n",
    "plt.xlabel(\"No. of Iteration\")\n",
    "plt.ylabel(\"LOSS\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 100])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-3c762892e242>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mlogps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;31m# TODO: Calculate the class probabilities (softmax) for img\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-0e1e6c9f3ba6>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m         )\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mused\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         return F.batch_norm(\n\u001b[0m\u001b[0;32m    168\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2277\u001b[0m         )\n\u001b[0;32m   2278\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2279\u001b[1;33m         \u001b[0m_verify_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2281\u001b[0m     return torch.batch_norm(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_verify_batch_size\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m   2245\u001b[0m         \u001b[0msize_prods\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2246\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_prods\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2247\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expected more than 1 value per channel when training, got input size {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 100])"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "\n",
    "# Test out your network!\n",
    "\n",
    "dataiter = iter(test_dataloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.resize_(1, 784)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "# TODO: Calculate the class probabilities (softmax) for img\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "# Plot the image and probabilities\n",
    "helper.view_classify(img.resize_(1, 28, 28), ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
